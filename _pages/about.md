---
permalink: /
title: "Paper reading record"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# Before 2024.4.8  
Jingze Su:  
(17'ICLR) Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer   
(23'CVPR) Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners  
(22'EMNLP) Mixture of Attention Heads: Selecting Attention Heads Per Token  
(23'ICCV) Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts  
(23'CVPR) Visual Prompt Multi-Modal Tracking   
(23'CVPR) Multimodal Prompting with Missing Modalities for Visual Recognition  
(23'Arxiv) Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation  
(24'CVPR) Single-Model and Any-Modality for Video Object Tracking
