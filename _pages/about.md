---
permalink: /
title: "Paper reading record"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## Before 2024.4
### Qi Li
(21'CVPR) Distilling Knowledge via Knowledge Review  
(23'ICCV) Lightweight Image Super-Resolution with Superpixel Token Interaction  
(23'ICCV) Multi-modal Gated Mixture of Local-to-Global Experts for Dynamic Image Fusion   
(23'ICCV) TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts
### Jiexin Luo
(21'CVPR) Multi-Scale Aligned Distillation for Low-Resolution Detection  
(21'CVPR) Channel-wise Knowledge Distillation for Dense Prediction  
(23'CVPR) EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention  
(23'ICCV) EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction  
(22'CVPR) Masked Autoencoders Are Scalable Vision Learners  
(24'CVPR) EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment  Anything  
### Jingze Su
(17'ICLR) Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer   
(23'CVPR) Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners  
(22'EMNLP) Mixture of Attention Heads: Selecting Attention Heads Per Token  
(23'ICCV) Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts  
(23'CVPR) Visual Prompt Multi-Modal Tracking   
(23'CVPR) Multimodal Prompting with Missing Modalities for Visual Recognition  
