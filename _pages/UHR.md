---
layout: archive
title: "Ultra-high resolution image understanding"
permalink: /UHR/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

### Playing to the Strengths of High-and Low-Resolution Cues for Ultra-high Resolution Image Segmentation
#### **<mark>Qi Li</mark>**, Jiexin Luo, Chunxiao Chen, Jiaxin Cai, Wenjie Yang, Yuanlong Yu, Shengfeng He, Wenxi Liu  
#### IEEE Robotics and Automation Letters (RA-L) [Paper](https://ieeexplore.ieee.org/abstract/document/11034711)  
**Abstract:** In ultra-high resolution image segmentation task for robotic platforms like AAVs and autonomous vehicles, existing paradigms process a downsampled input image through a deep network and the original high-resolution image through a shallow network, then fusing their features for final segmentation. Although these features are designed to be complementary, they often contain redundant or even conflicting semantic information, which leads to blurred edge contours, particularly for small objects. This is especially detrimental to robotics applications requiring precise spatial awareness. To address this challenge, we propose a novel paradigm that disentangles the task into two independent subtasks concerning high- and low-resolution inputs, leveraging high-resolution features exclusively to capture low-level structured details and low-resolution features for extracting semantics. Specifically, for the high-resolution input, we propose a region-pixel association experts scheme that partitions the image into multiple regions. For the low-resolution input, we assign compact semantic tokens to the partitioned regions. Additionally, we incorporate a high-resolution local perception scheme with an efficient field-enriched local context module to enhance small object recognition in case of incorrect semantic assignment. Extensive experiments demonstrate the state-of-the-art performance of our method and validate the effectiveness of each designed component.  
![framework4](https://github.com/QiLi2024/QiLi2024.github.io/blob/master/framework/RAL25.png)
### Memory-constrained semantic segmentation for ultra-high resolution UAV imagery
#### **<mark>Qi Li</mark>**, Jiaxin Cai, Jiexin Luo, Yuanlong Yu, Jason Gu, Jia Pan, Wenxi Liu  
#### IEEE Robotics and Automation Letters (RA-L) [Paper](https://ieeexplore.ieee.org/abstract/document/10380673) [Code](https://github.com/liqiokkk/SGHRQ)   
**Abstract:** Ultra-high resolution image segmentation poses a formidable challenge for UAVs with limited computation resources. Moreover, with multiple deployed tasks (e.g., mapping, localization, and decision making), the demand for a memory efficient model becomes more urgent. This letter delves into the intricate problem of achieving efficient and effective segmentation of ultra-high resolution UAV imagery, while operating under stringent GPU memory limitation. To address this problem, we propose a GPU memory-efficient and effective framework. Specifically, we introduce a novel and efficient spatial-guided high-resolution query module, which enables our model to effectively infer pixel-wise segmentation results by querying nearest latent embeddings from low-resolution features. Additionally, we present a memory-based interaction scheme with linear complexity to rectify semantic bias beneath the high-resolution spatial guidance via associating cross-image contextual semantics. For evaluation, we perform comprehensive experiments over public benchmarks under both conditions of small and large GPU memory usage limitations. Notably, our model gains around 3% advantage against SOTA in mIoU using comparable memory. Furthermore, we show that our model can be deployed on the embedded platform with less than 8 G memory like Jetson TX2.  
![framework3](https://github.com/QiLi2024/QiLi2024.github.io/blob/master/framework/RAL24.png)
### Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement
#### Wenxi Liu, **<mark>Qi Li</mark>**, Xindai Lin, Weixiang Yang, Shengfeng He, Yuanlong Yu  
#### International Journal of Computer Vision (IJCV) [Paper](https://link.springer.com/article/10.1007/s11263-024-02045-3)  
**Abstract:** Ultra-high resolution image segmentation has raised increasing interests in recent years due to its realistic applications. In this paper, we innovate the widely used high-resolution image segmentation pipeline, in which an ultra-high resolution image is partitioned into regular patches for local segmentation and then the local results are merged into a high-resolution semantic mask. In particular, we introduce a novel locality-aware context fusion based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the semantic regions with large variations. Additionally, we present the alternating local enhancement module that restricts the negative impact of redundant information introduced from the contexts, and thus is endowed with the ability of fixing the locality-aware features to produce refined results. Furthermore, in comprehensive experiments, we demonstrate that our model outperforms other state-of-the-art methods in public benchmarks and verify the effectiveness of the proposed modules.  
![framework2](https://github.com/QiLi2024/QiLi2024.github.io/blob/master/framework/IJCV24.png)
### From contexts to locality: Ultra-high resolution image segmentation via locality-aware contextual correlation
#### **<mark>Qi Li</mark>**, Weixiang Yang, Wenxi Liu, Yuanlong Yu, Shengfeng He  
#### ICCV [Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Li_From_Contexts_to_Locality_Ultra-High_Resolution_Image_Segmentation_via_Locality-Aware_ICCV_2021_paper.html) [Code](https://github.com/liqiokkk/FCtL)  
**Abstract:** Ultra-high resolution image segmentation has raised increasing interests in recent years due to its realistic applications. In this paper, we innovate the widely used high-resolution image segmentation pipeline, in which an ultra-high resolution image is partitioned into regular patches for local segmentation and then the local results are merged into a high-resolution semantic mask. In particular, we introduce a novel locality-aware contextual correlation based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the semantic regions with large variations. Additionally, we present a contextual semantics refinement network that associates the local segmentation result with its contextual semantics, and thus is endowed with the ability of reducing boundary artifacts and refining mask contours during the generation of final high-resolution mask. Furthermore, in comprehensive experiments, we demonstrate that our model outperforms other state-of-the-art methods in public benchmarks.  
![framework1](https://QiLi2024.github.io/framework/ICCV21.png)
